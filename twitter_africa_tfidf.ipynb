{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_tfidf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4lW699bvOYy"
      },
      "source": [
        "Clone the project repo containing the Twitter data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgmfzzhCmeq9",
        "outputId": "bc026276-b35b-4a0a-c434-1a4a730c1067"
      },
      "source": [
        "!git clone https://github.com/Data-Mining-2021/project.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'project'...\n",
            "remote: Enumerating objects: 194, done.\u001b[K\n",
            "remote: Counting objects: 100% (194/194), done.\u001b[K\n",
            "remote: Compressing objects: 100% (132/132), done.\u001b[K\n",
            "remote: Total 194 (delta 81), reused 116 (delta 43), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (194/194), 19.98 MiB | 10.82 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LiufTkbwGUb"
      },
      "source": [
        "Get all imports at once"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlDL7_2Kv3KF",
        "outputId": "3a463e19-09de-49ff-a7e4-d4d0b1e5d5ae"
      },
      "source": [
        "import pandas as pd\n",
        "!pip install langdetect\n",
        "from langdetect import detect\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from joblib import dump, load\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdQwLhvPw7yt"
      },
      "source": [
        "Generate kmeans cluster file from TFIDF vectorization of tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeqYGXjL7Pb_",
        "outputId": "36ab75ee-acee-4dc1-cf0f-5bac8912db8f"
      },
      "source": [
        "def tokenize_tweet(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    return filtered_tokens\n",
        "\n",
        "\n",
        "africa_df = pd.read_csv('/content/project/regions/Africa/africa_tweets_filtered.csv')\n",
        "\n",
        "africa_text = africa_df['Text']\n",
        "\n",
        "# vectorize into tfidf format (1-grams and 2-grams)\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=200000, max_df=0.8, tokenizer=tokenize_tweet, use_idf=True, stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(africa_text)\n",
        "\n",
        "# cluster using k-means++\n",
        "num_clusters = 5\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "km.fit(tfidf_matrix)\n",
        "clusters = km.labels_.tolist()\n",
        "\n",
        "# export cluster data\n",
        "dump(km, 'doc_cluster.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['doc_cluster.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmqBb6_-t91x"
      },
      "source": [
        "Organize TFIDF clusters into a dataframe w/ corresponding tweet data (username, country)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ7dDXdlwskI",
        "outputId": "94779f3e-6aa7-4bd1-c5ae-0c6282335aec"
      },
      "source": [
        "# reload the cluster data\n",
        "km = load('doc_cluster.pkl')\n",
        "clusters = km.labels_.tolist()\n",
        "\n",
        "# enter tfidf cluster data and corresponding twitter users into dataframe\n",
        "tweets = { 'cluster': clusters, 'username': africa_df['Username'].tolist(), 'country': africa_df['Country'].tolist() }\n",
        "cluster_df = pd.DataFrame(tweets, index = [clusters], columns = ['cluster', 'username', 'country'])\n",
        "\n",
        "# df['cluster'].value_counts() # number of tweets per cluster\n",
        "\n",
        "print('Cluster occurance based on country:')\n",
        "grouped = cluster_df['cluster'].groupby(cluster_df['country'])\n",
        "grouped.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster occurance based on country:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "country       cluster\n",
              "Liberia       0            30\n",
              "Nigeria       0          1005\n",
              "              4            33\n",
              "South Africa  0          4926\n",
              "              4          1216\n",
              "              1           350\n",
              "              2           214\n",
              "              3           135\n",
              "Name: cluster, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAJtv0zD24lJ"
      },
      "source": [
        "Display cluster data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvObaBpautH4",
        "outputId": "bd6efe6f-af6d-4c87-8940-f2934bb060c7"
      },
      "source": [
        "num_words = 10\n",
        "print(f'Top {num_words} terms per cluster:\\n')\n",
        "\n",
        "# sort cluster centers by proximity to centroid\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "for i in range(num_clusters):\n",
        "  print(f'Cluster {i} words:', end='')\n",
        "  for index in order_centroids[i, :n]:\n",
        "    print(f' {terms[index]},', end='')\n",
        "\n",
        "  print(f'\\nCluster {i} usernames:', end='')\n",
        "  for u in cluster_df.loc[i]['username'].unique():\n",
        "    print(f' {u},', end='')\n",
        "\n",
        "  print(f'\\nCluster {i} countries:', end='')\n",
        "  for c in cluster_df.loc[i]['country'].unique():\n",
        "    print(f' {c},', end='')\n",
        "\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 terms per cluster:\n",
            "\n",
            "Cluster 0 words: people, south, covid19, s, africa, need, country, government, amp, work,\n",
            "Cluster 0 usernames: CyrilRamaphosa, DrZweliMkhize, GeorgeWeahOff, MBuhari, femigbaja,\n",
            "Cluster 0 countries: South Africa, Liberia, Nigeria,\n",
            "\n",
            "Cluster 1 words: total number, total, number, number confirmed, number deaths, confirmed, deaths, cases total, recoveries, confirmed covid19,\n",
            "Cluster 1 usernames: DrZweliMkhize,\n",
            "Cluster 1 countries: South Africa,\n",
            "\n",
            "Cluster 2 words: app, sa, sa app, covid alert, alert sa, alert, covid, use covid, app protect, ones community,\n",
            "Cluster 2 usernames: DrZweliMkhize,\n",
            "Cluster 2 countries: South Africa,\n",
            "\n",
            "Cluster 3 words: statistics, covid19 statistics, covid19, statistics sa, sa, statistics south, sa august, july, sa july, august,\n",
            "Cluster 3 usernames: CyrilRamaphosa, DrZweliMkhize,\n",
            "Cluster 3 countries: South Africa,\n",
            "\n",
            "Cluster 4 words: health, covid19, dr, workers, healthcare, minister, listentotheexperts, vaccine, says, mkhize,\n",
            "Cluster 4 usernames: CyrilRamaphosa, DrZweliMkhize, MBuhari, femigbaja,\n",
            "Cluster 4 countries: South Africa, Nigeria,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}