{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_tfidf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4lW699bvOYy"
      },
      "source": [
        "Clone the project repo containing the Twitter data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgmfzzhCmeq9",
        "outputId": "e5ef8131-4256-4e32-e293-5e96fd6bb5ff"
      },
      "source": [
        "!git clone https://github.com/Data-Mining-2021/project.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'project' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LiufTkbwGUb"
      },
      "source": [
        "Get all imports at once"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlDL7_2Kv3KF",
        "outputId": "b6a67c2a-0918-4c5c-f71b-7d24c75c26d8"
      },
      "source": [
        "import pandas as pd\n",
        "!pip install langdetect\n",
        "from langdetect import detect\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from joblib import dump, load\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JUqQC4KPhTH"
      },
      "source": [
        "Filter Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r76nBwW5PaOf"
      },
      "source": [
        "# https://stackoverflow.com/a/49146722/330558\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "# filter out non-english tweets - also checks for NaN and tweets only containing emojis\n",
        "def filter_tweets(tweets_df):\n",
        "    for index in tweets_df.index:\n",
        "        currText = tweets_df.loc[index, 'Text']\n",
        "        if currText != currText or not remove_emoji(currText).strip() or detect(currText) != 'en':\n",
        "            tweets_df = tweets_df.drop([index])\n",
        "    return tweets_df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYOhLOPyruxG"
      },
      "source": [
        "Load in the data and then filter the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0yPfViErs3C"
      },
      "source": [
        "nontrump_df = pd.read_csv('/content/project/Results/all_tweets_notrump_blanksremoved.csv')\n",
        "alltweets_df = pd.read_csv('/content/project/Results/all_tweets.csv')\n",
        "trump_df = pd.read_csv('/content/project/Results/trump_tweets.csv')\n",
        "#nontrump_df = pd.read_csv('/content/project/all_tweets_notrump_blanksremoved.csv') #use later when clone isn't being a bitch\n",
        "\n",
        "#filter the dataframe for each data set\n",
        "nontrump_filtered_df = filter_tweets(nontrump_df)\n",
        "nontrump_text = nontrump_filtered_df['Text']\n",
        "\n",
        "alltweets_filtered_df = filter_tweets(alltweets_df)\n",
        "alltweets_text = alltweets_filtered_df['Text']\n",
        "\n",
        "trump_filtered_df = filter_tweets(trump_df)\n",
        "trump_text = trump_filtered_df['Text']\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdQwLhvPw7yt"
      },
      "source": [
        "Generate kmeans cluster file from TFIDF vectorization of tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeqYGXjL7Pb_"
      },
      "source": [
        "def tokenize_tweet(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token) and len(token) > 3:\n",
        "            filtered_tokens.append(token)\n",
        "    return filtered_tokens\n",
        "\n",
        "# vectorize into tfidf format (1-grams and 2-grams)\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=200000, max_df=0.8, tokenizer=tokenize_tweet, use_idf=True, stop_words='english')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5Uc-TWxbmvQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aeca547-a929-43c2-d7a3-60ac54514671"
      },
      "source": [
        "#perform Kmeans for the non-trump tweets\n",
        "tfidf_matrix_nontrump = vectorizer.fit_transform(nontrump_text)\n",
        "\n",
        "# cluster using k-means++\n",
        "num_clusters_nontrump = 7\n",
        "km = KMeans(n_clusters=num_clusters_nontrump)\n",
        "km.fit(tfidf_matrix_nontrump)\n",
        "clusters_nontrump = km.labels_.tolist()\n",
        "\n",
        "# export cluster data\n",
        "dump(km, 'nontrump_clusters_7.pkl')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nontrump_clusters_7.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNiEL9BlbnX0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca0851e-ed7f-4114-c070-c3df990f8b80"
      },
      "source": [
        "#perform Kmeans for the trump tweets with 7 and 10 clusters\n",
        "tfidf_matrix_trump = vectorizer.fit_transform(trump_text)\n",
        "\n",
        "# cluster using k-means++ with 7 clusters\n",
        "num_clusters_trump1 = 7\n",
        "km = KMeans(n_clusters=num_clusters_trump1)\n",
        "km.fit(tfidf_matrix_trump)\n",
        "clusters_trump1 = km.labels_.tolist()\n",
        "# export cluster data\n",
        "dump(km, 'trump_clusters_7.pkl')\n",
        "\n",
        "\n",
        "# cluster using k-means++ with 10 clusters\n",
        "num_clusters_trump2 = 10\n",
        "km = KMeans(n_clusters=num_clusters_trump2)\n",
        "km.fit(tfidf_matrix_trump)\n",
        "clusters_trump2 = km.labels_.tolist()\n",
        "# export cluster data\n",
        "dump(km, 'trump_clusters_10.pkl')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['trump_clusters_10.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kr4Z_uHboli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fba1a9f4-d0ec-4407-ee02-b94fd5685e2a"
      },
      "source": [
        "#perform Kmeans for the all tweets\n",
        "tfidf_matrix_all = vectorizer.fit_transform(alltweets_text)\n",
        "dump(tfidf_matrix_all, 'all_tweets_vectorized.pkl')\n",
        "\n",
        "# cluster using k-means++\n",
        "num_clusters_all = 7\n",
        "km = KMeans(n_clusters=num_clusters_all)\n",
        "km.fit(tfidf_matrix_all)\n",
        "clusters_all = km.labels_.tolist()\n",
        "\n",
        "# export cluster data\n",
        "dump(km, 'alltweets_clusters_7.pkl')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alltweets_clusters_7.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmqBb6_-t91x"
      },
      "source": [
        "Organize TFIDF clusters into a dataframe w/ corresponding tweet data (username, country)\n",
        "\n",
        "Also print the results for each data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXqgAIsWddmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9651d03-2309-4982-8df1-06a8f2c85064"
      },
      "source": [
        "#show cluster occurrence by country for non trump tweets\n",
        "# reload the cluster data\n",
        "km_nontrump = load('nontrump_clusters_7.pkl')\n",
        "clusters_nontrump = km_nontrump.labels_.tolist()\n",
        "\n",
        "#define the vocabulary\n",
        "nontrump_vocab = vectorizer.vocabulary_\n",
        "\n",
        "# enter tfidf cluster data and corresponding twitter users into dataframe\n",
        "tweets_nontrump = { 'cluster': clusters_nontrump, 'username': nontrump_filtered_df['Username'].tolist(), 'country': nontrump_filtered_df['Country'].tolist() }\n",
        "cluster_nontrump_df = pd.DataFrame(tweets_nontrump, index = [clusters_nontrump], columns = ['cluster', 'username', 'country'])\n",
        "\n",
        "print('Cluster occurrence based on country:')\n",
        "grouped_nontrump = cluster_nontrump_df['cluster']\n",
        "grouped_nontrump = cluster_nontrump_df['cluster'].groupby(cluster_nontrump_df['country'])\n",
        "grouped_nontrump.value_counts()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster occurrence based on country:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "country       cluster\n",
              "Australia     0          1001\n",
              "              2           223\n",
              "              5             1\n",
              "              6             1\n",
              "Canada        0          4645\n",
              "              6           892\n",
              "              2           479\n",
              "              5           106\n",
              "Chile         0           308\n",
              "              2            42\n",
              "India         0           534\n",
              "              2             6\n",
              "Israel        0          2944\n",
              "              2           230\n",
              "              6           205\n",
              "              5             1\n",
              "Liberia       0            29\n",
              "              2             1\n",
              "New Zealand   0           308\n",
              "              2            14\n",
              "Nigeria       0           984\n",
              "              2            53\n",
              "South Africa  0          5556\n",
              "              2           708\n",
              "              4           357\n",
              "              1           209\n",
              "              5             4\n",
              "South Korea   0           198\n",
              "              2             1\n",
              "UN            0          2572\n",
              "              2           138\n",
              "USA           0          8109\n",
              "              2           586\n",
              "              5           429\n",
              "              3           285\n",
              "Name: cluster, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4cmVmBss0VF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afc2f906-84e0-4506-bb50-8fc70ece583e"
      },
      "source": [
        "#display the cluster data for nontrump tweets\n",
        "num_words = 10\n",
        "print(f'Top {num_words} terms per cluster:\\n')\n",
        "\n",
        "# sort cluster centers by proximity to centroid\n",
        "order_centroids = km_nontrump.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "for i in range(num_clusters_nontrump):\n",
        "  print(f'Cluster {i} words:', end='')\n",
        "  for index in order_centroids[i, :num_words]:\n",
        "    print(f' {terms[index]},', end='')\n",
        "\n",
        "  print(f'\\nCluster {i} usernames:', end='')\n",
        "  for u in cluster_nontrump_df.loc[i]['username'].unique():\n",
        "    print(f' {u},', end='')\n",
        "\n",
        "  print(f'\\nCluster {i} countries:', end='')\n",
        "  for c in cluster_nontrump_df.loc[i]['country'].unique():\n",
        "    print(f' {c},', end='')\n",
        "\n",
        "  print('\\n')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 terms per cluster:\n",
            "\n",
            "Cluster 0 words: priority- encouraged, global covax, trained respected, parties usually, reached 50th, work public, future storms, fight york, touch member, times realistic,\n",
            "Cluster 0 usernames: JoeBiden, SpeakerPelosi, LeaderMcConnell, NYGovCuomo, GovRonDeSantis, Canada, JustinTrudeau, CanadianPM, fordnation, CyrilRamaphosa, DrZweliMkhize, GeorgeWeahOff, MBuhari, femigbaja, PresidentRuvi, IsraeliPM, nsitharaman, TheBlueHouseENG, ScottMorrisonMP, GregHuntMP, jacindaardern, AndrewLittleMP, mbachelet, antonioguterres, AminaJMohammed, volkan_bozkir,\n",
            "Cluster 0 countries: USA, Canada, South Africa, Liberia, Nigeria, Israel, India, South Korea, Australia, New Zealand, Chile, UN,\n",
            "\n",
            "Cluster 1 words: fulfil, ahppc, fugitive, aimed, place challenging, succesful conclusion, country second, reimbursements, reach health, veterans guide,\n",
            "Cluster 1 usernames: DrZweliMkhize,\n",
            "Cluster 1 countries: South Africa,\n",
            "\n",
            "Cluster 2 words: nation especially, businesses local, nation faith, reminder simple, removal head, workersmemorialday honors, ohio including, ohio leaving, ones festive, global covax,\n",
            "Cluster 2 usernames: JoeBiden, SpeakerPelosi, LeaderMcConnell, NYGovCuomo, GovRonDeSantis, Canada, JustinTrudeau, CanadianPM, fordnation, CyrilRamaphosa, DrZweliMkhize, GeorgeWeahOff, MBuhari, femigbaja, PresidentRuvi, IsraeliPM, nsitharaman, TheBlueHouseENG, ScottMorrisonMP, GregHuntMP, AndrewLittleMP, mbachelet, antonioguterres, AminaJMohammed, volkan_bozkir,\n",
            "Cluster 2 countries: USA, Canada, South Africa, Liberia, Nigeria, Israel, India, South Korea, Australia, New Zealand, Chile, UN,\n",
            "\n",
            "Cluster 3 words: truly control, yesterday memory, time wonderful, value water, quality remember, treatment eventual, moment leaders, necessary stabilize, services thank, performative outrage,\n",
            "Cluster 3 usernames: NYGovCuomo,\n",
            "Cluster 3 countries: USA,\n",
            "\n",
            "Cluster 4 words: truly make, perceived adverse, truly control, percentage people, perdue filibuster, decisively, declare state, help public, review outcomes, campaign want,\n",
            "Cluster 4 usernames: DrZweliMkhize,\n",
            "Cluster 4 countries: South Africa,\n",
            "\n",
            "Cluster 5 words: week extra, week iowa, number worry, bipartisan program, navy shipbuilding, navy ships, realistic., numbers focus, realities night, moving case,\n",
            "Cluster 5 usernames: JoeBiden, SpeakerPelosi, NYGovCuomo, GovRonDeSantis, JustinTrudeau, CanadianPM, DrZweliMkhize, IsraeliPM, GregHuntMP,\n",
            "Cluster 5 countries: USA, Canada, South Africa, Israel, Australia,\n",
            "\n",
            "Cluster 6 words: receive approval, receive additional, ones festive, ones sharing, understand appreciate, nigeria unfair, nigeria undergone, traveler, week extra, rules residents,\n",
            "Cluster 6 usernames: JustinTrudeau, CanadianPM, fordnation, IsraeliPM, ScottMorrisonMP,\n",
            "Cluster 6 countries: Canada, Israel, Australia,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r-6JuhmdeJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa8f4c7-32b8-46a4-c5c4-e5f8d8edcb7a"
      },
      "source": [
        "#show cluster occurrence by country for trump tweets\n",
        "# reload the cluster data\n",
        "km_trump = load('trump_clusters_7.pkl')\n",
        "clusters_trump = km_trump.labels_.tolist()\n",
        "\n",
        "#define the vocabulary\n",
        "trump_vocab = vectorizer.vocabulary_\n",
        "\n",
        "# enter tfidf cluster data and corresponding twitter users into dataframe\n",
        "tweets_trump = { 'cluster': clusters_trump, 'username': trump_filtered_df['Username'].tolist(), 'country': trump_filtered_df['Country'].tolist() }\n",
        "cluster_trump_df = pd.DataFrame(tweets_trump, index = [clusters_trump], columns = ['cluster', 'username', 'country'])\n",
        "\n",
        "print('Cluster occurrence based on country:')\n",
        "grouped_trump = cluster_trump_df['cluster']\n",
        "grouped_trump = cluster_trump_df['cluster'].groupby(cluster_trump_df['country'])\n",
        "grouped_trump.value_counts()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster occurrence based on country:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "country  cluster\n",
              "Trump    5          3648\n",
              "         0           459\n",
              "         1           301\n",
              "         3           237\n",
              "         2           192\n",
              "         6            93\n",
              "         4            27\n",
              "Name: cluster, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2kAKClNtCY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6265f67-a479-4966-8072-67964f36a089"
      },
      "source": [
        "#display the cluster data for trump tweets\n",
        "num_words = 10\n",
        "print(f'Top {num_words} terms per cluster:\\n')\n",
        "\n",
        "# sort cluster centers by proximity to centroid\n",
        "order_centroids = km_trump.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "for i in range(num_clusters_trump1):\n",
        "  print(f'Cluster {i} words:', end='')\n",
        "  for index in order_centroids[i, :num_words]:\n",
        "    print(f' {terms[index]},', end='')\n",
        "\n",
        "  print(f'\\nCluster {i} usernames:', end='')\n",
        "  for u in cluster_trump_df.loc[i]['username'].unique():\n",
        "    print(f' {u},', end='')\n",
        "\n",
        "  print(f'\\nCluster {i} countries:', end='')\n",
        "  for c in cluster_trump_df.loc[i]['country'].unique():\n",
        "    print(f' {c},', end='')\n",
        "\n",
        "  print('\\n')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 terms per cluster:\n",
            "\n",
            "Cluster 0 words: arguing, faith family, islamabad president-elect, isixhosa, madam, coming suburbs, maddox prize, fake media, muafak, musa mthombheni,\n",
            "Cluster 0 usernames: theRealDonaldTrump,\n",
            "Cluster 0 countries: Trump,\n",
            "\n",
            "Cluster 1 words: correct automobile, corporations provide, great victory, fully support, green deal, expertise, environments, experts, boris johnson, fattah al-burhan,\n",
            "Cluster 1 usernames: theRealDonaldTrump,\n",
            "Cluster 1 countries: Trump,\n",
            "\n",
            "Cluster 2 words: mmjoshi murli_manohar_joshi, following necessary, mobilized communities, ahead today, israel wear, ahead path, israelelections israelex4, keeping promise, helped wuhan, help stop,\n",
            "Cluster 2 usernames: theRealDonaldTrump,\n",
            "Cluster 2 countries: Trump,\n",
            "\n",
            "Cluster 3 words: campaign finance, cooperate need, campaign powered, monitor ongoing, monitor developments, administered doses, limiting, limited vaccine, moving c_mulroney, message appeared,\n",
            "Cluster 3 usernames: theRealDonaldTrump,\n",
            "Cluster 3 countries: Trump,\n",
            "\n",
            "Cluster 4 words: days pandemic, dear friends, distributed equitably, covid19 changed, covid19 contact, covid19 consider, covid19 confinement, covid19 communities, covid19 come, covid19 climatechange,\n",
            "Cluster 4 usernames: theRealDonaldTrump,\n",
            "Cluster 4 countries: Trump,\n",
            "\n",
            "Cluster 5 words: days pandemic, convened high-level, history honor, environments, ceremony group, muafak, mmjoshi murli_manohar_joshi, including covid19, mooikloof mega, massive election,\n",
            "Cluster 5 usernames: theRealDonaldTrump,\n",
            "Cluster 5 countries: Trump,\n",
            "\n",
            "Cluster 6 words: heightened risk, canada second, greater economic, narendramodi sections, narendramodi recognised, doing safe, heinous attack, continuing invest, great victory, monarchs leaders,\n",
            "Cluster 6 usernames: theRealDonaldTrump,\n",
            "Cluster 6 countries: Trump,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBzex5TZe8Zd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4808704-7fb9-4023-dd06-e2e96df5accb"
      },
      "source": [
        "#show cluster occurrence by country for all tweets\n",
        "# reload the cluster data\n",
        "km_alltweets = load('alltweets_clusters_7.pkl')\n",
        "clusters_alltweets = km_alltweets.labels_.tolist()\n",
        "\n",
        "#define the vocabulary\n",
        "alltweets_vocab = vectorizer.vocabulary_\n",
        "\n",
        "# enter tfidf cluster data and corresponding twitter users into dataframe\n",
        "tweets_alltweets = { 'cluster': clusters_alltweets, 'username': alltweets_filtered_df['Username'].tolist(), 'country': alltweets_filtered_df['Country'].tolist() }\n",
        "cluster_alltweets_df = pd.DataFrame(tweets_alltweets, index = [clusters_alltweets], columns = ['cluster', 'username', 'country'])\n",
        "\n",
        "print('Cluster occurrence based on country:')\n",
        "grouped_alltweets = cluster_alltweets_df['cluster']\n",
        "grouped_alltweets = cluster_alltweets_df['cluster'].groupby(cluster_alltweets_df['country'])\n",
        "grouped_alltweets.value_counts()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster occurrence based on country:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "country       cluster\n",
              "Australia     1          1224\n",
              "              4             1\n",
              "              5             1\n",
              "Canada        1          4933\n",
              "              6           985\n",
              "              3           188\n",
              "              4            11\n",
              "Chile         1           350\n",
              "India         1           540\n",
              "Israel        1          2565\n",
              "              4           813\n",
              "              3             1\n",
              "Liberia       1            30\n",
              "New Zealand   1           320\n",
              "              4             2\n",
              "Nigeria       1          1037\n",
              "              3             1\n",
              "South Africa  1          6253\n",
              "              0           367\n",
              "              2           209\n",
              "              5             4\n",
              "              4             1\n",
              "South Korea   1           199\n",
              "Trump         1          4912\n",
              "              3            31\n",
              "              4             3\n",
              "              5             2\n",
              "UN            1          2709\n",
              "              3             1\n",
              "              4             1\n",
              "USA           1          8046\n",
              "              3           649\n",
              "              5           431\n",
              "              0           285\n",
              "              4             1\n",
              "Name: cluster, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShQydqJatT4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d26248ee-b1b5-41c9-9231-0897e6447ec5"
      },
      "source": [
        "#display the cluster data for all tweets\n",
        "num_words = 10\n",
        "print(f'Top {num_words} terms per cluster:\\n')\n",
        "\n",
        "# sort cluster centers by proximity to centroid\n",
        "order_centroids = km_alltweets.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "for i in range(num_clusters_all):\n",
        "  print(f'Cluster {i} words:', end='')\n",
        "  for index in order_centroids[i, :num_words]:\n",
        "    print(f' {terms[index]},', end='')\n",
        "\n",
        "  print(f'\\nCluster {i} usernames:', end='')\n",
        "  for u in cluster_alltweets_df.loc[i]['username'].unique():\n",
        "    print(f' {u},', end='')\n",
        "\n",
        "  print(f'\\nCluster {i} countries:', end='')\n",
        "  for c in cluster_alltweets_df.loc[i]['country'].unique():\n",
        "    print(f' {c},', end='')\n",
        "\n",
        "  print('\\n')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 terms per cluster:\n",
            "\n",
            "Cluster 0 words: total, total number, number, yesterday, today, number confirmed, number deaths, confirmed, confirmed covid19, deaths,\n",
            "Cluster 0 usernames: NYGovCuomo, DrZweliMkhize,\n",
            "Cluster 0 countries: USA, South Africa,\n",
            "\n",
            "Cluster 1 words: people, covid19, today, thank, health, great, need, president, country, work,\n",
            "Cluster 1 usernames: JoeBiden, SpeakerPelosi, LeaderMcConnell, NYGovCuomo, GovRonDeSantis, Canada, JustinTrudeau, CanadianPM, fordnation, CyrilRamaphosa, DrZweliMkhize, GeorgeWeahOff, MBuhari, femigbaja, PresidentRuvi, IsraeliPM, nsitharaman, TheBlueHouseENG, ScottMorrisonMP, GregHuntMP, jacindaardern, AndrewLittleMP, mbachelet, antonioguterres, AminaJMohammed, volkan_bozkir, theRealDonaldTrump,\n",
            "Cluster 1 countries: USA, Canada, South Africa, Liberia, Nigeria, Israel, India, South Korea, Australia, New Zealand, Chile, UN, Trump,\n",
            "\n",
            "Cluster 2 words: covid alert, alert, covid, alert protect, ones community, start using, community start, protect loved, privacy preserving, fight download,\n",
            "Cluster 2 usernames: DrZweliMkhize,\n",
            "Cluster 2 countries: South Africa,\n",
            "\n",
            "Cluster 3 words: donald, donald trump, trump, premier ford, ford, premier, provides covid-19, ford provides, covid-19 update, make announcement,\n",
            "Cluster 3 usernames: JoeBiden, SpeakerPelosi, NYGovCuomo, fordnation, MBuhari, IsraeliPM, antonioguterres, theRealDonaldTrump,\n",
            "Cluster 3 countries: USA, Canada, Nigeria, Israel, UN, Trump,\n",
            "\n",
            "Cluster 4 words: minister, prime minister, prime, netanyahu, benjamin netanyahu, benjamin, minister benjamin, minister netanyahu, israel, remarks,\n",
            "Cluster 4 usernames: SpeakerPelosi, JustinTrudeau, CanadianPM, fordnation, CyrilRamaphosa, PresidentRuvi, IsraeliPM, ScottMorrisonMP, AndrewLittleMP, AminaJMohammed, theRealDonaldTrump,\n",
            "Cluster 4 countries: USA, Canada, South Africa, Israel, Australia, New Zealand, UN, Trump,\n",
            "\n",
            "Cluster 5 words: watch, briefing, watch live, live, holding, holding briefing, press, live press, press conference, giving daily,\n",
            "Cluster 5 usernames: JoeBiden, SpeakerPelosi, NYGovCuomo, GovRonDeSantis, DrZweliMkhize, GregHuntMP, theRealDonaldTrump,\n",
            "Cluster 5 countries: USA, South Africa, Australia, Trump,\n",
            "\n",
            "Cluster 6 words: minister justin, justin trudeau, trudeau, justin, prime minister, prime, minister, watch, today prime, live prime,\n",
            "Cluster 6 usernames: CanadianPM,\n",
            "Cluster 6 countries: Canada,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1az0v9M7hCRe"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEvClTOzXU4j"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuYByBQidhth"
      },
      "source": [
        "cluster_colors = {0: '#ff0000', 1: '#ffff00', 2: '#00ff00', 3: '#00ffff', 4: '#0000ff', 5: '#ff00ff', 6: '#888888'}\n",
        "\n",
        "\n",
        "import os  # for os.path.basename\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "from sklearn.manifold import MDS\n",
        "\n",
        "MDS()\n",
        "\n",
        "# convert two components as we're plotting points in a two-dimensional plane\n",
        "# \"precomputed\" because we provide a distance matrix\n",
        "# we will also specify `random_state` so the plot is reproducible.\n",
        "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
        "\n",
        "pos = mds.fit_transform(1-cosine_similarity(tfidf_matrix_all))  # shape (n_components, n_samples)\n",
        "\n",
        "xs, ys = pos[:, 0], pos[:, 1]\n",
        "print()\n",
        "print()\n",
        "clusters = km_alltweets.labels_.tolist()\n",
        "#some ipython magic to show the matplotlib plots inline\n",
        "%matplotlib inline \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VshL36Uzivzy"
      },
      "source": [
        "xs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6vMARKLi0UQ"
      },
      "source": [
        "ys.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEB_Neirir1m"
      },
      "source": [
        "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
        "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters)) \n",
        "\n",
        "#group by cluster\n",
        "groups = df.groupby('label')\n",
        "\n",
        "\n",
        "# set up plot\n",
        "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
        "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
        "\n",
        "#iterate through groups to layer the plot\n",
        "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
        "for name, group in groups:\n",
        "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
        "            label=cluster_names[name], color=cluster_colors[name], \n",
        "            mec='none')\n",
        "    ax.set_aspect('auto')\n",
        "    ax.tick_params(\\\n",
        "        axis= 'x',          # changes apply to the x-axis\n",
        "        which='both',      # both major and minor ticks are affected\n",
        "        bottom='off',      # ticks along the bottom edge are off\n",
        "        top='off',         # ticks along the top edge are off\n",
        "        labelbottom='off')\n",
        "    ax.tick_params(\\\n",
        "        axis= 'y',         # changes apply to the y-axis\n",
        "        which='both',      # both major and minor ticks are affected\n",
        "        left='off',      # ticks along the bottom edge are off\n",
        "        top='off',         # ticks along the top edge are off\n",
        "        labelleft='off')\n",
        "    \n",
        "ax.legend(numpoints=1)  #show legend with only 1 point\n",
        "\n",
        "#add label in x,y position with the label as the film title\n",
        "for i in range(len(df)):\n",
        "    ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=8)  \n",
        "\n",
        "    \n",
        "    \n",
        "plt.show() #show the plot\n",
        "\n",
        "#uncomment the below to save the plot if need be\n",
        "#plt.savefig('clusters_small_noaxes.png', dpi=200)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}